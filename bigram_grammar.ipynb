{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNTHX85yLbVqTA4kZTQVxrY"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFDjJ1Bpimtg",
        "outputId": "d4e71910-571f-40f1-976b-ac5aa5b4d0c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+--------------------+--------------+-----------------+\n",
            "| Original Word   | Original POS Tag   | Error Word   | Error POS Tag   |\n",
            "+=================+====================+==============+=================+\n",
            "| The             | DT                 | The          | DT              |\n",
            "+-----------------+--------------------+--------------+-----------------+\n",
            "| current         | JJ                 | curreent     | JJ              |\n",
            "+-----------------+--------------------+--------------+-----------------+\n",
            "| world           | NN                 | world        | NN              |\n",
            "+-----------------+--------------------+--------------+-----------------+\n",
            "| situation       | NN                 | situation    | NN              |\n",
            "+-----------------+--------------------+--------------+-----------------+\n",
            "| demands         | VBZ                | demands      | VBZ             |\n",
            "+-----------------+--------------------+--------------+-----------------+\n",
            "| global          | JJ                 | gloobal      | JJ              |\n",
            "+-----------------+--------------------+--------------+-----------------+\n",
            "| cooperation     | NN                 | cooperation  | NN              |\n",
            "+-----------------+--------------------+--------------+-----------------+\n",
            "| and             | CC                 | and          | CC              |\n",
            "+-----------------+--------------------+--------------+-----------------+\n",
            "| immediate       | JJ                 | immmediate   | JJ              |\n",
            "+-----------------+--------------------+--------------+-----------------+\n",
            "| action          | NN                 | action       | NN              |\n",
            "+-----------------+--------------------+--------------+-----------------+\n",
            "| .               | .                  | .            | .               |\n",
            "+-----------------+--------------------+--------------+-----------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tag import pos_tag_sents\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from tabulate import tabulate\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "# Original sentence\n",
        "sentence = \"The current world situation demands global cooperation and immediate action.\"\n",
        "# Tokenize the sentence into words\n",
        "sent = sent_tokenize(sentence)\n",
        "tokenized_words = [word_tokenize(s) for s in sent]\n",
        "# Perform POS tagging on the tokenized words\n",
        "tagged_words = pos_tag_sents(tokenized_words)\n",
        "\n",
        "# Error sentence\n",
        "error_sentence = \"The curreent world situation demands gloobal cooperation and immmediate action.\"\n",
        "# Tokenize the error sentence into words\n",
        "error_sent = sent_tokenize(error_sentence)\n",
        "tokenized_error_words = [word_tokenize(s) for s in error_sent]\n",
        "# Perform POS tagging on the error sentence\n",
        "tagged_error_words = pos_tag_sents(tokenized_error_words)\n",
        "\n",
        "# Preparing table to print in a visually pleasing way\n",
        "table = []\n",
        "for original_word, error_word, original_tag, error_tag in zip(tokenized_words[0], tokenized_error_words[0],\n",
        "                                                              [t[1] for t in tagged_words[0]],\n",
        "                                                              [t[1] for t in tagged_error_words[0]]):\n",
        "    table.append([original_word, original_tag, error_word, error_tag])\n",
        "\n",
        "# Print the table\n",
        "print(tabulate(table, headers=[\"Original Word\", \"Original POS Tag\", \"Error Word\", \"Error POS Tag\"], tablefmt=\"grid\"))\n"
      ]
    }
  ]
}